#!/usr/bin/env python3
"""
æ•°æ®é›†åŠ è½½æµ‹è¯•è„šæœ¬
æµ‹è¯• FLOATDataset çš„å„é¡¹åŠŸèƒ½ï¼ŒåŒ…æ‹¬æ•°æ®åŠ è½½ã€é¢„å¤„ç†å’Œæ•°æ®å®Œæ•´æ€§æ£€æŸ¥
"""

import os
import sys
import torch
import traceback
import argparse
from pathlib import Path
from torch.utils.data import DataLoader
import time
import json

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from training.dataset import FLOATDataset, create_dataloader
from options.base_options import BaseOptions


class DatasetTester:
    """æ•°æ®é›†æµ‹è¯•å™¨"""
    
    def __init__(self, config_path=None):
        """
        åˆå§‹åŒ–æµ‹è¯•å™¨
        
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°
        """
        if config_path and os.path.exists(config_path):
            self.opt = self.load_config_from_json(config_path)
        else:
            self.opt = BaseOptions().parse()
        
        self.test_results = {
            'dataset_creation': False,
            'data_loading': False,
            'batch_loading': False,
            'data_shapes': False,
            'data_types': False,
            'error_handling': False,
            'performance': {}
        }
    
    def load_config_from_json(self, config_path):
        """ä»JSONé…ç½®æ–‡ä»¶åŠ è½½é…ç½®"""
        with open(config_path, 'r', encoding='utf-8') as f:
            config = json.load(f)
        
        # åˆ›å»ºä¸€ä¸ªç®€å•çš„é…ç½®å¯¹è±¡
        class Config:
            pass
        
        opt = Config()
        
        # åˆå¹¶æ‰€æœ‰é…ç½®é¡¹
        for section in config.values():
            if isinstance(section, dict):
                for key, value in section.items():
                    setattr(opt, key, value)
        
        return opt
    
    def test_dataset_creation(self):
        """æµ‹è¯•æ•°æ®é›†åˆ›å»º"""
        print("ğŸ” æµ‹è¯•æ•°æ®é›†åˆ›å»º...")
        try:
            # æ£€æŸ¥å¿…è¦çš„è·¯å¾„
            if not hasattr(self.opt, 'data_root'):
                print("âŒ é…ç½®ä¸­ç¼ºå°‘ data_root")
                return False
            
            if not os.path.exists(self.opt.data_root):
                print(f"âŒ æ•°æ®æ ¹ç›®å½•ä¸å­˜åœ¨: {self.opt.data_root}")
                return False
            
            # åˆ›å»ºæ•°æ®é›†
            dataset = FLOATDataset(
                data_root=self.opt.data_root,
                train=True,
                opt=self.opt
            )
            
            print(f"âœ… æ•°æ®é›†åˆ›å»ºæˆåŠŸ")
            print(f"   æ•°æ®é›†å¤§å°: {len(dataset)}")
            
            if len(dataset) == 0:
                print("âš ï¸  è­¦å‘Š: æ•°æ®é›†ä¸ºç©º")
                return False
            
            self.dataset = dataset
            self.test_results['dataset_creation'] = True
            return True
            
        except Exception as e:
            print(f"âŒ æ•°æ®é›†åˆ›å»ºå¤±è´¥: {str(e)}")
            print(f"   é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            return False
    
    def test_single_data_loading(self):
        """æµ‹è¯•å•ä¸ªæ•°æ®é¡¹åŠ è½½"""
        print("\nğŸ” æµ‹è¯•å•ä¸ªæ•°æ®é¡¹åŠ è½½...")
        try:
            if not hasattr(self, 'dataset'):
                print("âŒ æ•°æ®é›†æœªåˆ›å»º")
                return False
            
            # æµ‹è¯•åŠ è½½ç¬¬ä¸€ä¸ªæ•°æ®é¡¹
            start_time = time.time()
            data_item = self.dataset[0]
            load_time = time.time() - start_time
            
            print(f"âœ… å•ä¸ªæ•°æ®é¡¹åŠ è½½æˆåŠŸ")
            print(f"   åŠ è½½æ—¶é—´: {load_time:.2f}ç§’")
            
            # æ£€æŸ¥æ•°æ®é¡¹ç»“æ„
            expected_keys = [
                'video_cur', 'video_prev', 'motion_latent_cur', 'motion_latent_prev',
                'audio_latent_cur', 'audio_latent_prev', 'reference_frame', 
                'reference_motion', 'emotion_features', 'actor_id'
            ]
            
            missing_keys = [key for key in expected_keys if key not in data_item]
            if missing_keys:
                print(f"âš ï¸  ç¼ºå°‘é”®: {missing_keys}")
            else:
                print("âœ… æ•°æ®é¡¹åŒ…å«æ‰€æœ‰å¿…è¦çš„é”®")
            
            self.sample_data = data_item
            self.test_results['data_loading'] = True
            self.test_results['performance']['single_load_time'] = load_time
            return True
            
        except Exception as e:
            print(f"âŒ å•ä¸ªæ•°æ®é¡¹åŠ è½½å¤±è´¥: {str(e)}")
            print(f"   é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            return False
    
    def test_data_shapes_and_types(self):
        """æµ‹è¯•æ•°æ®å½¢çŠ¶å’Œç±»å‹"""
        print("\nğŸ” æµ‹è¯•æ•°æ®å½¢çŠ¶å’Œç±»å‹...")
        try:
            if not hasattr(self, 'sample_data'):
                print("âŒ æ²¡æœ‰æ ·æœ¬æ•°æ®")
                return False
            
            data = self.sample_data
            
            # æ£€æŸ¥å„é¡¹æ•°æ®çš„å½¢çŠ¶å’Œç±»å‹
            checks = [
                ('video_cur', torch.Tensor, 4),  # (T, C, H, W)
                ('video_prev', torch.Tensor, 4),  # (T, C, H, W)
                ('motion_latent_cur', torch.Tensor, 2),  # (T, motion_dim)
                ('motion_latent_prev', torch.Tensor, 2),  # (T, motion_dim)
                ('audio_latent_cur', torch.Tensor, 2),  # (T, audio_dim)
                ('audio_latent_prev', torch.Tensor, 2),  # (T, audio_dim)
                ('reference_frame', torch.Tensor, 3),  # (C, H, W)
                ('reference_motion', torch.Tensor, 1),  # (motion_dim,)
                ('emotion_features', torch.Tensor, 1),  # (emotion_dim,)
            ]
            
            all_passed = True
            for key, expected_type, expected_dims in checks:
                if key in data:
                    value = data[key]
                    if not isinstance(value, expected_type):
                        print(f"âŒ {key}: ç±»å‹é”™è¯¯ï¼ŒæœŸæœ› {expected_type}ï¼Œå®é™… {type(value)}")
                        all_passed = False
                    elif value.dim() != expected_dims:
                        print(f"âŒ {key}: ç»´åº¦é”™è¯¯ï¼ŒæœŸæœ› {expected_dims}Dï¼Œå®é™… {value.dim()}D")
                        all_passed = False
                    else:
                        print(f"âœ… {key}: å½¢çŠ¶ {tuple(value.shape)}, ç±»å‹ {type(value)}")
                else:
                    print(f"âš ï¸  ç¼ºå°‘é”®: {key}")
                    all_passed = False
            
            # æ£€æŸ¥æ•°å€¼èŒƒå›´
            if 'video_cur' in data:
                video_min, video_max = data['video_cur'].min(), data['video_cur'].max()
                print(f"   è§†é¢‘åƒç´ å€¼èŒƒå›´: [{video_min:.3f}, {video_max:.3f}]")
                if video_min < -2 or video_max > 2:
                    print("âš ï¸  è§†é¢‘åƒç´ å€¼èŒƒå›´å¼‚å¸¸ï¼ŒæœŸæœ›åœ¨ [-1, 1] é™„è¿‘")
            
            self.test_results['data_shapes'] = all_passed
            self.test_results['data_types'] = all_passed
            return all_passed

        except Exception as e:
            print(f"âŒ æ•°æ®å½¢çŠ¶å’Œç±»å‹æ£€æŸ¥å¤±è´¥: {str(e)}")
            return False

    def test_batch_loading(self):
        """æµ‹è¯•æ‰¹é‡æ•°æ®åŠ è½½"""
        print("\nğŸ” æµ‹è¯•æ‰¹é‡æ•°æ®åŠ è½½...")
        try:
            if not hasattr(self, 'dataset'):
                print("âŒ æ•°æ®é›†æœªåˆ›å»º")
                return False

            batch_size = min(2, len(self.dataset))  # ä½¿ç”¨å°æ‰¹é‡è¿›è¡Œæµ‹è¯•

            dataloader = DataLoader(
                self.dataset,
                batch_size=batch_size,
                shuffle=False,
                num_workers=0,  # é¿å…å¤šè¿›ç¨‹é—®é¢˜
                pin_memory=False
            )

            start_time = time.time()
            batch = next(iter(dataloader))
            batch_load_time = time.time() - start_time

            print(f"âœ… æ‰¹é‡æ•°æ®åŠ è½½æˆåŠŸ")
            print(f"   æ‰¹æ¬¡å¤§å°: {batch_size}")
            print(f"   æ‰¹é‡åŠ è½½æ—¶é—´: {batch_load_time:.2f}ç§’")

            # æ£€æŸ¥æ‰¹é‡æ•°æ®å½¢çŠ¶
            for key, value in batch.items():
                if isinstance(value, torch.Tensor):
                    print(f"   {key}: {tuple(value.shape)}")
                else:
                    print(f"   {key}: {type(value)} (é•¿åº¦: {len(value) if hasattr(value, '__len__') else 'N/A'})")

            self.test_results['batch_loading'] = True
            self.test_results['performance']['batch_load_time'] = batch_load_time
            return True

        except Exception as e:
            print(f"âŒ æ‰¹é‡æ•°æ®åŠ è½½å¤±è´¥: {str(e)}")
            print(f"   é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            return False

    def test_error_handling(self):
        """æµ‹è¯•é”™è¯¯å¤„ç†"""
        print("\nğŸ” æµ‹è¯•é”™è¯¯å¤„ç†...")
        try:
            # æµ‹è¯•æ— æ•ˆç´¢å¼•
            try:
                invalid_data = self.dataset[len(self.dataset)]
                print("âš ï¸  æ— æ•ˆç´¢å¼•æœªæŠ›å‡ºå¼‚å¸¸")
                return False
            except IndexError:
                print("âœ… æ— æ•ˆç´¢å¼•æ­£ç¡®æŠ›å‡º IndexError")

            # æµ‹è¯•è´Ÿç´¢å¼•
            try:
                negative_data = self.dataset[-1]
                print("âœ… è´Ÿç´¢å¼•å¤„ç†æ­£å¸¸")
            except Exception as e:
                print(f"âš ï¸  è´Ÿç´¢å¼•å¤„ç†å¼‚å¸¸: {e}")

            self.test_results['error_handling'] = True
            return True

        except Exception as e:
            print(f"âŒ é”™è¯¯å¤„ç†æµ‹è¯•å¤±è´¥: {str(e)}")
            return False

    def test_performance(self):
        """æµ‹è¯•æ€§èƒ½"""
        print("\nğŸ” æµ‹è¯•æ€§èƒ½...")
        try:
            if not hasattr(self, 'dataset'):
                print("âŒ æ•°æ®é›†æœªåˆ›å»º")
                return False

            # æµ‹è¯•å¤šä¸ªæ•°æ®é¡¹çš„åŠ è½½æ—¶é—´
            num_samples = min(5, len(self.dataset))

            start_time = time.time()
            for i in range(num_samples):
                _ = self.dataset[i]
            total_time = time.time() - start_time

            avg_time = total_time / num_samples
            print(f"âœ… æ€§èƒ½æµ‹è¯•å®Œæˆ")
            print(f"   æµ‹è¯•æ ·æœ¬æ•°: {num_samples}")
            print(f"   æ€»æ—¶é—´: {total_time:.2f}ç§’")
            print(f"   å¹³å‡æ¯ä¸ªæ ·æœ¬: {avg_time:.2f}ç§’")

            self.test_results['performance']['avg_sample_time'] = avg_time
            self.test_results['performance']['total_test_time'] = total_time

            # æ€§èƒ½å»ºè®®
            if avg_time > 5.0:
                print("âš ï¸  è­¦å‘Š: æ•°æ®åŠ è½½è¾ƒæ…¢ï¼Œå»ºè®®æ£€æŸ¥æ•°æ®é¢„å¤„ç†æˆ–ä½¿ç”¨æ›´å¿«çš„å­˜å‚¨")
            elif avg_time > 2.0:
                print("âš ï¸  æ³¨æ„: æ•°æ®åŠ è½½æ—¶é—´è¾ƒé•¿ï¼Œå¯èƒ½å½±å“è®­ç»ƒæ•ˆç‡")
            else:
                print("âœ… æ•°æ®åŠ è½½æ€§èƒ½è‰¯å¥½")

            return True

        except Exception as e:
            print(f"âŒ æ€§èƒ½æµ‹è¯•å¤±è´¥: {str(e)}")
            return False

    def run_all_tests(self):
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        print("ğŸš€ å¼€å§‹æ•°æ®é›†æµ‹è¯•...")
        print("=" * 60)

        tests = [
            ('æ•°æ®é›†åˆ›å»º', self.test_dataset_creation),
            ('å•ä¸ªæ•°æ®åŠ è½½', self.test_single_data_loading),
            ('æ•°æ®å½¢çŠ¶å’Œç±»å‹', self.test_data_shapes_and_types),
            ('æ‰¹é‡æ•°æ®åŠ è½½', self.test_batch_loading),
            ('é”™è¯¯å¤„ç†', self.test_error_handling),
            ('æ€§èƒ½æµ‹è¯•', self.test_performance),
        ]

        passed_tests = 0
        total_tests = len(tests)

        for test_name, test_func in tests:
            try:
                if test_func():
                    passed_tests += 1
            except Exception as e:
                print(f"âŒ {test_name} æµ‹è¯•å¼‚å¸¸: {str(e)}")

        print("\n" + "=" * 60)
        print("ğŸ“Š æµ‹è¯•ç»“æœæ±‡æ€»:")
        print(f"   é€šè¿‡æµ‹è¯•: {passed_tests}/{total_tests}")
        print(f"   æˆåŠŸç‡: {passed_tests/total_tests*100:.1f}%")

        # è¯¦ç»†ç»“æœ
        print("\nğŸ“‹ è¯¦ç»†ç»“æœ:")
        for key, value in self.test_results.items():
            if key != 'performance':
                status = "âœ… é€šè¿‡" if value else "âŒ å¤±è´¥"
                print(f"   {key}: {status}")

        # æ€§èƒ½ä¿¡æ¯
        if self.test_results['performance']:
            print("\nâ±ï¸  æ€§èƒ½ä¿¡æ¯:")
            for key, value in self.test_results['performance'].items():
                print(f"   {key}: {value:.2f}ç§’")

        return passed_tests == total_tests


def main():
    parser = argparse.ArgumentParser(description='FLOATæ•°æ®é›†æµ‹è¯•è„šæœ¬')
    parser.add_argument('--config', type=str, default='training/example_config.json',
                       help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--data_root', type=str,
                       help='æ•°æ®æ ¹ç›®å½• (è¦†ç›–é…ç½®æ–‡ä»¶ä¸­çš„è®¾ç½®)')

    args = parser.parse_args()

    # åˆ›å»ºæµ‹è¯•å™¨
    tester = DatasetTester(args.config)

    # å¦‚æœæŒ‡å®šäº†data_rootï¼Œè¦†ç›–é…ç½®
    if args.data_root:
        tester.opt.data_root = args.data_root

    # è¿è¡Œæµ‹è¯•
    success = tester.run_all_tests()

    if success:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼æ•°æ®é›†åŠ è½½åŠŸèƒ½æ­£å¸¸ã€‚")
        sys.exit(0)
    else:
        print("\nğŸ’¥ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ•°æ®é›†é…ç½®å’Œæ•°æ®æ–‡ä»¶ã€‚")
        sys.exit(1)


if __name__ == "__main__":
    main()

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# FLOAT 模型推理代码详细解析\n",
        "\n",
        "本 notebook 对 `generate.py` 文件进行逐行详细解析，重点说明每个变量的形状（shape）和数据流转过程。\n",
        "\n",
        "## 概述\n",
        "\n",
        "`generate.py` 是 FLOAT 模型的推理脚本，主要功能是：\n",
        "1. 加载预训练的 FLOAT 模型\n",
        "2. 处理输入的参考图像和音频文件\n",
        "3. 生成驱动的面部视频\n",
        "4. 保存输出结果\n",
        "\n",
        "## 主要组件\n",
        "\n",
        "- **DataProcessor**: 负责图像和音频的预处理\n",
        "- **InferenceAgent**: 负责模型加载和推理过程\n",
        "- **InferenceOptions**: 负责命令行参数解析\n",
        "\n",
        "让我们逐步分析每个组件...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. 导入模块和依赖项\n",
        "\n",
        "首先我们来看代码的导入部分：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Inference Stage 2\n",
        "\"\"\"\n",
        "\n",
        "import os, torch, random, cv2, torchvision, subprocess, librosa, datetime, tempfile, face_alignment\n",
        "import numpy as np\n",
        "import albumentations as A\n",
        "import albumentations.pytorch.transforms as A_pytorch\n",
        "\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "from transformers import Wav2Vec2FeatureExtractor\n",
        "\n",
        "from models.float.FLOAT import FLOAT\n",
        "from options.base_options import BaseOptions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 导入模块说明：\n",
        "\n",
        "- **torch**: PyTorch 深度学习框架\n",
        "- **cv2**: OpenCV 图像处理库\n",
        "- **librosa**: 音频处理库\n",
        "- **face_alignment**: 人脸对齐和检测库\n",
        "- **albumentations**: 图像增强库\n",
        "- **transformers**: Hugging Face 的 Wav2Vec2 特征提取器\n",
        "- **FLOAT**: 主要的生成模型\n",
        "- **BaseOptions**: 命令行参数解析基类\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. DataProcessor 类详细解析\n",
        "\n",
        "`DataProcessor` 类负责图像和音频的预处理，是整个推理流程的第一步。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DataProcessor:\n",
        "\tdef __init__(self, opt):\n",
        "\t\tself.opt = opt\n",
        "\t\tself.fps = opt.fps                    # 帧率，默认25.0\n",
        "\t\tself.sampling_rate = opt.sampling_rate  # 音频采样率，默认16000\n",
        "\t\tself.input_size = opt.input_size      # 输入图像尺寸，默认512\n",
        "\n",
        "\t\t# 人脸对齐工具，用于检测人脸并获取关键点\n",
        "\t\tself.fa = face_alignment.FaceAlignment(face_alignment.LandmarksType.TWO_D, flip_input=False)\n",
        "\n",
        "\t\t# wav2vec2 音频预处理器，用于将音频转换为模型可接受的格式\n",
        "\t\tself.wav2vec_preprocessor = Wav2Vec2FeatureExtractor.from_pretrained(opt.wav2vec_model_path, local_files_only=True)\n",
        "\n",
        "\t\t# 图像变换管道\n",
        "\t\tself.transform = A.Compose([\n",
        "\t\t\t\tA.Resize(height=opt.input_size, width=opt.input_size, interpolation=cv2.INTER_AREA),  # 调整尺寸到512x512\n",
        "\t\t\t\tA.Normalize(mean=(0.5,0.5,0.5), std=(0.5,0.5,0.5)),  # 归一化到[-1,1]范围\n",
        "\t\t\t\tA_pytorch.ToTensorV2(),  # 转换为PyTorch张量\n",
        "\t\t\t])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1 图像处理函数 process_img\n",
        "\n",
        "这个函数负责人脸检测、裁剪和预处理：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def process_img(self, img: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    输入: img - 原始图像 numpy数组，shape: (H, W, 3)\n",
        "    输出: crop_img - 处理后的图像，shape: (input_size, input_size, 3)\n",
        "    \"\"\"\n",
        "    # 计算缩放倍数，将图像高度调整到360像素\n",
        "    mult = 360. / img.shape[0]  # mult: float, 缩放倍数\n",
        "    \n",
        "    # 按比例缩放图像\n",
        "    resized_img = cv2.resize(img, dsize=(0, 0), fx=mult, fy=mult, \n",
        "                            interpolation=cv2.INTER_AREA if mult < 1. else cv2.INTER_CUBIC)\n",
        "    # resized_img shape: (360, int(W*mult), 3)\n",
        "    \n",
        "    # 人脸检测，返回边界框列表\n",
        "    bboxes = self.fa.face_detector.detect_from_image(resized_img)\n",
        "    # bboxes: list of (x1, y1, x2, y2, score)\n",
        "    \n",
        "    # 过滤置信度低的检测结果，并将坐标还原到原图尺度\n",
        "    bboxes = [(int(x1 / mult), int(y1 / mult), int(x2 / mult), int(y2 / mult), score) \n",
        "              for (x1, y1, x2, y2, score) in bboxes if score > 0.95]\n",
        "    bboxes = bboxes[0]  # 只使用第一个检测到的人脸\n",
        "    \n",
        "    # 计算边界框的中心点和尺寸\n",
        "    bsy = int((bboxes[3] - bboxes[1]) / 2)  # 边界框高度的一半\n",
        "    bsx = int((bboxes[2] - bboxes[0]) / 2)  # 边界框宽度的一半  \n",
        "    my = int((bboxes[1] + bboxes[3]) / 2)   # 边界框中心y坐标\n",
        "    mx = int((bboxes[0] + bboxes[2]) / 2)   # 边界框中心x坐标\n",
        "    \n",
        "    # 确定裁剪区域大小（取长宽最大值的1.6倍）\n",
        "    bs = int(max(bsy, bsx) * 1.6)  # bs: int, 裁剪半径\n",
        "    \n",
        "    # 给图像添加边框，防止裁剪时越界\n",
        "    img = cv2.copyMakeBorder(img, bs, bs, bs, bs, cv2.BORDER_CONSTANT, value=0)\n",
        "    # img shape: (H+2*bs, W+2*bs, 3)\n",
        "    \n",
        "    # 更新中心坐标（考虑添加的边框）\n",
        "    my, mx = my + bs, mx + bs\n",
        "    \n",
        "    # 裁剪人脸区域\n",
        "    crop_img = img[my - bs:my + bs, mx - bs:mx + bs]\n",
        "    # crop_img shape: (2*bs, 2*bs, 3)\n",
        "    \n",
        "    # 调整到目标尺寸\n",
        "    crop_img = cv2.resize(crop_img, dsize=(self.input_size, self.input_size), \n",
        "                         interpolation=cv2.INTER_AREA if mult < 1. else cv2.INTER_CUBIC)\n",
        "    # crop_img shape: (512, 512, 3)\n",
        "    \n",
        "    return crop_img\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 音频处理函数\n",
        "\n",
        "音频加载和预处理函数：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def default_aud_loader(self, path: str) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    音频加载和预处理函数\n",
        "    输入: path - 音频文件路径\n",
        "    输出: torch.Tensor - 预处理后的音频特征，shape: (audio_length,)\n",
        "    \"\"\"\n",
        "    # 使用librosa加载音频，重采样到指定采样率\n",
        "    speech_array, sampling_rate = librosa.load(path, sr=self.sampling_rate)\n",
        "    # speech_array shape: (audio_length,) - 1D音频信号\n",
        "    # sampling_rate: int - 实际采样率（应该等于self.sampling_rate=16000）\n",
        "    \n",
        "    # 使用Wav2Vec2特征提取器处理音频\n",
        "    processed_audio = self.wav2vec_preprocessor(\n",
        "        speech_array, \n",
        "        sampling_rate=sampling_rate, \n",
        "        return_tensors='pt'\n",
        "    ).input_values[0]\n",
        "    # processed_audio shape: (audio_length,) - 经过预处理的音频张量\n",
        "    \n",
        "    return processed_audio\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.3 综合预处理函数\n",
        "\n",
        "将图像和音频预处理整合在一起：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess(self, ref_path: str, audio_path: str, no_crop: bool) -> dict:\n",
        "    \"\"\"\n",
        "    综合预处理函数\n",
        "    输入:\n",
        "        ref_path: str - 参考图像路径\n",
        "        audio_path: str - 音频文件路径  \n",
        "        no_crop: bool - 是否跳过人脸裁剪\n",
        "    输出:\n",
        "        dict - 包含预处理后数据的字典\n",
        "    \"\"\"\n",
        "    # 加载并处理参考图像\n",
        "    s = self.default_img_loader(ref_path)  # s shape: (H, W, 3)\n",
        "    \n",
        "    if not no_crop:\n",
        "        s = self.process_img(s)  # s shape: (512, 512, 3)\n",
        "    \n",
        "    # 应用图像变换（归一化、转tensor等）\n",
        "    s = self.transform(image=s)['image'].unsqueeze(0)\n",
        "    # s shape: (1, 3, 512, 512) - 添加batch维度\n",
        "    \n",
        "    # 加载并处理音频\n",
        "    a = self.default_aud_loader(audio_path).unsqueeze(0)\n",
        "    # a shape: (1, audio_length) - 添加batch维度\n",
        "    \n",
        "    # 返回数据字典\n",
        "    return {\n",
        "        's': s,     # 图像张量 (1, 3, 512, 512)\n",
        "        'a': a,     # 音频张量 (1, audio_length)  \n",
        "        'p': None,  # 姿态信息（此处为None）\n",
        "        'e': None   # 表情信息（此处为None）\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. InferenceAgent 类详细解析\n",
        "\n",
        "`InferenceAgent` 类是推理的核心，负责模型加载、权重加载和执行推理过程。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class InferenceAgent:\n",
        "    def __init__(self, opt):\n",
        "        torch.cuda.empty_cache()  # 清理GPU缓存\n",
        "        self.opt = opt\n",
        "        self.rank = opt.rank  # GPU设备ID，默认为0\n",
        "        \n",
        "        # 加载模型架构\n",
        "        self.load_model()\n",
        "        \n",
        "        # 加载预训练权重\n",
        "        self.load_weight(opt.ckpt_path, rank=self.rank)\n",
        "        \n",
        "        # 将模型移动到指定设备并设置为评估模式\n",
        "        self.G.to(self.rank)  # self.G是FLOAT模型实例\n",
        "        self.G.eval()\n",
        "        \n",
        "        # 初始化数据处理器\n",
        "        self.data_processor = DataProcessor(opt)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 模型加载函数\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_model(self) -> None:\n",
        "    \"\"\"\n",
        "    加载FLOAT模型架构\n",
        "    创建模型实例但不加载权重\n",
        "    \"\"\"\n",
        "    self.G = FLOAT(self.opt)  # 创建FLOAT模型实例\n",
        "    # self.G 包含以下主要组件：\n",
        "    # - motion_autoencoder: 运动潜在自编码器\n",
        "    # - audio_encoder: 音频编码器  \n",
        "    # - emotion_encoder: 情感编码器\n",
        "    # - fmt: 流匹配变换器 (Flow Matching Transformer)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 权重加载函数\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_weight(self, checkpoint_path: str, rank: int) -> None:\n",
        "    \"\"\"\n",
        "    从检查点文件加载模型权重\n",
        "    输入:\n",
        "        checkpoint_path: str - 检查点文件路径\n",
        "        rank: int - GPU设备ID\n",
        "    \"\"\"\n",
        "    # 加载状态字典（权重参数）\n",
        "    state_dict = torch.load(checkpoint_path, map_location='cpu', weights_only=True)\n",
        "    # state_dict: dict - 包含所有模型参数的字典\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        # 遍历模型的所有参数\n",
        "        for model_name, model_param in self.G.named_parameters():\n",
        "            if model_name in state_dict:\n",
        "                # 如果参数在检查点中存在，则加载\n",
        "                model_param.copy_(state_dict[model_name].to(rank))\n",
        "                # model_param shape: 根据具体参数而定\n",
        "            elif \"wav2vec2\" in model_name: \n",
        "                # wav2vec2参数通常预训练好，跳过\n",
        "                pass\n",
        "            else:\n",
        "                # 警告：参数未找到\n",
        "                print(f\"! Warning; {model_name} not found in state_dict.\")\n",
        "    \n",
        "    del state_dict  # 清理内存\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3 视频保存函数\n",
        "\n",
        "这个函数负责将生成的视频张量保存为MP4文件：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_video(self, vid_target_recon: torch.Tensor, video_path: str, audio_path: str) -> str:\n",
        "    \"\"\"\n",
        "    将生成的视频张量保存为MP4文件\n",
        "    输入:\n",
        "        vid_target_recon: torch.Tensor - 生成的视频张量，shape: (T, 3, H, W) 或 (1, T, 3, H, W)\n",
        "        video_path: str - 输出视频路径\n",
        "        audio_path: str - 音频文件路径（用于合成）\n",
        "    输出:\n",
        "        str - 保存的视频文件路径\n",
        "    \"\"\"\n",
        "    with tempfile.NamedTemporaryFile(suffix='.mp4', delete=False) as temp_video:\n",
        "        temp_filename = temp_video.name\n",
        "        \n",
        "        print(f\"保存视频，输入tensor形状: {vid_target_recon.shape}, 设备: {vid_target_recon.device}\")\n",
        "        \n",
        "        # 确保tensor在CPU上进行后续处理\n",
        "        if vid_target_recon.device.type != 'cpu':\n",
        "            vid = vid_target_recon.detach().cpu()\n",
        "        else:\n",
        "            vid = vid_target_recon.detach()\n",
        "        \n",
        "        # 处理维度：如果是3D tensor (T, C, H, W)，需要添加batch维度\n",
        "        if vid.dim() == 4:\n",
        "            vid = vid.unsqueeze(0)  # 添加batch维度: (1, T, C, H, W)\n",
        "        \n",
        "        # 转换维度顺序为 (N, T, H, W, C) 用于torchvision视频写入\n",
        "        vid = vid.permute(0, 1, 3, 4, 2)\n",
        "        # vid shape: (1, T, H, W, 3)\n",
        "        \n",
        "        # 将像素值从[-1,1]范围转换到[0,255]范围\n",
        "        vid = vid.clamp(-1, 1)  # 确保在[-1,1]范围内\n",
        "        vid = ((vid + 1) / 2 * 255).type('torch.ByteTensor')\n",
        "        # vid shape: (1, T, H, W, 3), dtype: uint8, range: [0,255]\n",
        "        \n",
        "        print(f\"写入视频，处理后形状: {vid.shape}\")\n",
        "        \n",
        "        # 使用torchvision写入视频文件\n",
        "        torchvision.io.write_video(temp_filename, vid.squeeze(0), fps=self.opt.fps)\n",
        "        \n",
        "        # 如果提供了音频文件，使用ffmpeg合成音视频\n",
        "        if audio_path is not None:\n",
        "            with open(os.devnull, 'wb') as f:\n",
        "                command = \"ffmpeg -i {} -i {} -c:v copy -c:a aac {} -y\".format(\n",
        "                    temp_filename, audio_path, video_path)\n",
        "                subprocess.call(command, shell=True, stdout=f, stderr=f)\n",
        "            \n",
        "            # 删除临时文件\n",
        "            if os.path.exists(video_path):\n",
        "                os.remove(temp_filename)\n",
        "        else:\n",
        "            # 如果没有音频，直接重命名临时文件\n",
        "            os.rename(temp_filename, video_path)\n",
        "        \n",
        "        return video_path\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.4 核心推理函数\n",
        "\n",
        "这是整个推理流程的核心函数：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def run_inference(\n",
        "    self,\n",
        "    res_video_path: str,    # 输出视频路径\n",
        "    ref_path: str,          # 参考图像路径\n",
        "    audio_path: str,        # 音频文件路径\n",
        "    a_cfg_scale: float = 2.0,  # 音频引导缩放因子\n",
        "    r_cfg_scale: float = 1.0,  # 参考引导缩放因子\n",
        "    e_cfg_scale: float = 1.0,  # 情感引导缩放因子\n",
        "    emo: str = 'S2E',          # 情感标签\n",
        "    nfe: int = 10,             # ODE求解器步数\n",
        "    no_crop: bool = False,     # 是否跳过裁剪\n",
        "    seed: int = 25,            # 随机种子\n",
        "    verbose: bool = True       # 是否打印详细信息\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    执行完整的推理流程\n",
        "    \n",
        "    数据流程：\n",
        "    1. 预处理 -> data dict\n",
        "    2. FLOAT模型推理 -> d_hat tensor  \n",
        "    3. 保存视频 -> 输出文件\n",
        "    \"\"\"\n",
        "    \n",
        "    # 步骤1: 数据预处理\n",
        "    data = self.data_processor.preprocess(ref_path, audio_path, no_crop=no_crop)\n",
        "    # data: dict = {\n",
        "    #     's': torch.Tensor,  # 图像 (1, 3, 512, 512)\n",
        "    #     'a': torch.Tensor,  # 音频 (1, audio_length)\n",
        "    #     'p': None,          # 姿态（未使用）\n",
        "    #     'e': None           # 表情（未使用）\n",
        "    # }\n",
        "    \n",
        "    if verbose: print(f\"> [Done] Preprocess.\")\n",
        "    \n",
        "    # 步骤2: 模型推理\n",
        "    d_hat = self.G.inference(\n",
        "        data=data,\n",
        "        a_cfg_scale=a_cfg_scale,\n",
        "        r_cfg_scale=r_cfg_scale, \n",
        "        e_cfg_scale=e_cfg_scale,\n",
        "        emo=emo,\n",
        "        nfe=nfe,\n",
        "        seed=seed\n",
        "    )['d_hat']\n",
        "    # d_hat shape: (T, 3, 512, 512) - 生成的视频序列\n",
        "    # T: 视频帧数，由音频长度和fps决定\n",
        "    \n",
        "    # 步骤3: 保存视频\n",
        "    res_video_path = self.save_video(d_hat, res_video_path, audio_path)\n",
        "    \n",
        "    if verbose: print(f\"> [Done] result saved at {res_video_path}\")\n",
        "    return res_video_path\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. FLOAT 模型内部推理过程详解\n",
        "\n",
        "让我们深入了解 `self.G.inference()` 内部的处理流程和各个变量的形状变化：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# FLOAT.inference() 方法的详细流程\n",
        "\n",
        "def inference(self, data: dict, a_cfg_scale=None, r_cfg_scale=None, e_cfg_scale=None, \n",
        "              emo=None, nfe=10, seed=None) -> dict:\n",
        "    \"\"\"\n",
        "    FLOAT模型的推理过程\n",
        "    \n",
        "    输入数据流转：\n",
        "    data['s']: (1, 3, 512, 512) -> 参考图像\n",
        "    data['a']: (1, audio_length) -> 音频序列\n",
        "    \n",
        "    内部处理流程：\n",
        "    1. 图像编码 -> 潜在表示\n",
        "    2. 音频编码 -> 音频特征  \n",
        "    3. 情感编码 -> 情感特征\n",
        "    4. 流匹配采样 -> 运动序列\n",
        "    5. 解码 -> 视频帧序列\n",
        "    \"\"\"\n",
        "    \n",
        "    # 提取输入数据\n",
        "    s, a = data['s'], data['a']  \n",
        "    # s shape: (1, 3, 512, 512) - 参考图像\n",
        "    # a shape: (1, audio_length) - 音频序列\n",
        "    \n",
        "    # === 步骤1: 图像编码到潜在空间 ===\n",
        "    s_r, r_s_lambda, s_r_feats = self.encode_image_into_latent(s.to(self.opt.rank))\n",
        "    # s_r shape: (1, dim_w) - 图像潜在表示，dim_w=512\n",
        "    # r_s_lambda shape: (1, dim_w) - 身份潜在表示\n",
        "    # s_r_feats: list - 编码器中间特征，用于解码\n",
        "    \n",
        "    # === 步骤2: 计算身份方向向量 ===\n",
        "    if 's_r' in data:\n",
        "        r_s = self.encode_identity_into_motion(s_r)\n",
        "    else:\n",
        "        r_s = self.motion_autoencoder.dec.direction(r_s_lambda)\n",
        "    # r_s shape: (1, dim_w) - 身份运动方向，dim_w=512\n",
        "    \n",
        "    data['r_s'] = r_s\n",
        "    \n",
        "    # === 步骤3: 设置引导缩放参数 ===\n",
        "    if a_cfg_scale is None: a_cfg_scale = self.opt.a_cfg_scale  # 默认2.0\n",
        "    if r_cfg_scale is None: r_cfg_scale = self.opt.r_cfg_scale  # 默认1.0  \n",
        "    if e_cfg_scale is None: e_cfg_scale = self.opt.e_cfg_scale  # 默认1.0\n",
        "    \n",
        "    # === 步骤4: 流匹配采样生成运动序列 ===\n",
        "    sample = self.sample(\n",
        "        data, \n",
        "        a_cfg_scale=a_cfg_scale, \n",
        "        r_cfg_scale=r_cfg_scale, \n",
        "        e_cfg_scale=e_cfg_scale, \n",
        "        emo=emo, \n",
        "        nfe=nfe, \n",
        "        seed=seed\n",
        "    )\n",
        "    # sample shape: (1, T, dim_w) - 生成的运动序列\n",
        "    # T: 视频总帧数，由音频长度决定\n",
        "    \n",
        "    # === 步骤5: 解码潜在表示到视频帧 ===\n",
        "    data_out = self.decode_latent_into_image(s_r=s_r, s_r_feats=s_r_feats, r_d=sample)\n",
        "    # data_out['d_hat'] shape: (T, 3, 512, 512) - 最终生成的视频序列\n",
        "    \n",
        "    return data_out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.1 流匹配采样过程 (sample 方法)\n",
        "\n",
        "这是FLOAT模型的核心生成过程：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# FLOAT.sample() 方法详解\n",
        "\n",
        "def sample(self, data: dict, a_cfg_scale: float = 1.0, r_cfg_scale: float = 1.0, \n",
        "           e_cfg_scale: float = 1.0, emo: str = None, nfe: int = 10, seed: int = None) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    流匹配采样生成运动序列\n",
        "    \"\"\"\n",
        "    \n",
        "    r_s, a = data['r_s'], data['a']\n",
        "    # r_s shape: (1, dim_w) - 身份特征，dim_w=512\n",
        "    # a shape: (1, audio_length) - 音频序列\n",
        "    \n",
        "    B = a.shape[0]  # B=1, batch size\n",
        "    \n",
        "    # === 音频处理 ===\n",
        "    # 计算视频总帧数T\n",
        "    T = math.ceil(a.shape[-1] * self.opt.fps / self.opt.sampling_rate)\n",
        "    # T = ceil(audio_length * 25.0 / 16000) - 视频总帧数\n",
        "    print(\"T =\", T)\n",
        "    \n",
        "    # 音频编码：将整段音频编码为帧级特征\n",
        "    a = a.to(self.opt.rank)\n",
        "    wa = self.audio_encoder.inference(a, seq_len=T)\n",
        "    # wa shape: (1, T, dim_w) - 音频特征序列，每帧对应一个特征向量\n",
        "    \n",
        "    # === 情感处理 ===\n",
        "    emo_idx = self.emotion_encoder.label2id.get(str(emo).lower(), None)\n",
        "    if emo_idx is None:\n",
        "        # 自动预测情感\n",
        "        we = self.emotion_encoder.predict_emotion(a).unsqueeze(1)\n",
        "        # we shape: (1, 1, dim_e) - 情感概率分布，dim_e=7\n",
        "    else:\n",
        "        # 使用指定情感\n",
        "        we = F.one_hot(torch.tensor(emo_idx, device=a.device), num_classes=self.opt.dim_e).unsqueeze(0).unsqueeze(0)\n",
        "        # we shape: (1, 1, dim_e) - one-hot编码的情感向量\n",
        "    \n",
        "    # === 分块采样 ===\n",
        "    sample = []\n",
        "    num_frames_for_clip = int(self.opt.wav2vec_sec * self.opt.fps)  # 每个clip的帧数，默认50帧\n",
        "    num_prev_frames = int(self.opt.num_prev_frames)  # 前序帧数，默认10帧\n",
        "    \n",
        "    # 逐块处理视频序列\n",
        "    for t in tqdm(range(0, int(math.ceil(T / num_frames_for_clip))), desc=\"Sampling\"):\n",
        "        \n",
        "        # === 噪声初始化 ===\n",
        "        if self.opt.fix_noise_seed:\n",
        "            seed = self.opt.seed if seed is None else seed\n",
        "            g = torch.Generator(self.opt.rank)\n",
        "            g.manual_seed(seed)\n",
        "            x0 = torch.randn(B, num_frames_for_clip, self.opt.dim_w, device=self.opt.rank, generator=g)\n",
        "        else:\n",
        "            x0 = torch.randn(B, num_frames_for_clip, self.opt.dim_w, device=self.opt.rank)\n",
        "        # x0 shape: (1, 50, dim_w) - 初始噪声，dim_w=512\n",
        "        \n",
        "        # === 前序帧处理 ===\n",
        "        if t == 0:  # 第一个clip，前序帧为零\n",
        "            prev_x_t = torch.zeros(B, num_prev_frames, self.opt.dim_w).to(self.opt.rank)\n",
        "            prev_wa_t = torch.zeros(B, num_prev_frames, self.opt.dim_w).to(self.opt.rank)\n",
        "        else:  # 使用前一个clip的最后几帧作为前序帧\n",
        "            prev_x_t = sample_t[:, -num_prev_frames:]\n",
        "            prev_wa_t = wa_t[:, -num_prev_frames:]\n",
        "        # prev_x_t shape: (1, 10, dim_w) - 前序运动特征\n",
        "        # prev_wa_t shape: (1, 10, dim_w) - 前序音频特征\n",
        "        \n",
        "        # === 当前clip的音频特征 ===\n",
        "        wa_t = wa[:, t * num_frames_for_clip: (t+1) * num_frames_for_clip]\n",
        "        # wa_t shape: (1, 50, dim_w) - 当前clip的音频特征\n",
        "        \n",
        "        # 如果不足50帧，进行填充\n",
        "        if wa_t.shape[1] < num_frames_for_clip:\n",
        "            wa_t = F.pad(wa_t, (0, 0, 0, num_frames_for_clip - wa_t.shape[1]), mode='replicate')\n",
        "        \n",
        "        # === ODE求解函数 ===\n",
        "        def sample_chunk(tt, zt):\n",
        "            \"\"\"\n",
        "            ODE求解的右侧函数\n",
        "            tt: 时间步，shape: (1,)\n",
        "            zt: 当前状态，shape: (1, 50, dim_w)\n",
        "            \"\"\"\n",
        "            out = self.fmt.forward_with_cfv(\n",
        "                t=tt.unsqueeze(0),      # 时间步 (1, 1)\n",
        "                x=zt,                   # 当前状态 (1, 50, dim_w)\n",
        "                wa=wa_t,                # 音频特征 (1, 50, dim_w)\n",
        "                wr=r_s,                 # 身份特征 (1, dim_w)\n",
        "                we=we,                  # 情感特征 (1, 1, dim_e)\n",
        "                prev_x=prev_x_t,        # 前序运动 (1, 10, dim_w)\n",
        "                prev_wa=prev_wa_t,      # 前序音频 (1, 10, dim_w)\n",
        "                a_cfg_scale=a_cfg_scale,\n",
        "                r_cfg_scale=r_cfg_scale,\n",
        "                e_cfg_scale=e_cfg_scale\n",
        "            )\n",
        "            # out shape: (1, 60, dim_w) - 包含前序帧+当前帧\n",
        "            \n",
        "            out_current = out[:, num_prev_frames:]  # 只取当前帧部分\n",
        "            # out_current shape: (1, 50, dim_w)\n",
        "            return out_current\n",
        "        \n",
        "        # === 使用ODE求解器进行采样 ===\n",
        "        time = torch.linspace(0, 1, self.opt.nfe, device=self.opt.rank)  # 时间网格\n",
        "        trajectory_t = odeint(sample_chunk, x0, time, **self.odeint_kwargs)\n",
        "        # trajectory_t shape: (nfe, 1, 50, dim_w) - 整个轨迹\n",
        "        \n",
        "        sample_t = trajectory_t[-1]  # 取最后一个时间步的结果\n",
        "        # sample_t shape: (1, 50, dim_w) - 当前clip的采样结果\n",
        "        \n",
        "        sample.append(sample_t)\n",
        "    \n",
        "    # === 拼接所有clip ===\n",
        "    sample = torch.cat(sample, dim=1)[:, :T]  # 截取到实际帧数T\n",
        "    # sample shape: (1, T, dim_w) - 完整的运动序列\n",
        "    \n",
        "    return sample\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2 解码过程 (decode_latent_into_image)\n",
        "\n",
        "将运动序列解码回视频帧：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def decode_latent_into_image(self, s_r: torch.Tensor, s_r_feats: list, r_d: torch.Tensor, batch_size: int = 50) -> dict:\n",
        "    \"\"\"\n",
        "    将潜在运动序列解码为视频帧\n",
        "    \n",
        "    输入:\n",
        "        s_r: torch.Tensor - 参考图像的潜在表示，shape: (1, dim_w)\n",
        "        s_r_feats: list - 编码器中间特征，用于跳跃连接\n",
        "        r_d: torch.Tensor - 运动序列，shape: (1, T, dim_w)\n",
        "        batch_size: int - 批处理大小，防止GPU内存溢出\n",
        "    \n",
        "    输出:\n",
        "        dict - 包含生成视频的字典，{'d_hat': torch.Tensor}\n",
        "    \"\"\"\n",
        "    \n",
        "    T = r_d.shape[1]  # T: 视频总帧数\n",
        "    d_hat_list = []   # 存储分批处理的结果\n",
        "    \n",
        "    print(f\"开始分批解码，总帧数: {T}, 批处理大小: {batch_size}\")\n",
        "    \n",
        "    # 分批处理，避免GPU内存不足\n",
        "    for start_idx in range(0, T, batch_size):\n",
        "        end_idx = min(start_idx + batch_size, T)\n",
        "        batch_frames = []\n",
        "        \n",
        "        print(f\"处理帧 {start_idx} 到 {end_idx-1}\")\n",
        "        \n",
        "        # 逐帧解码当前批次\n",
        "        for t in range(start_idx, end_idx):\n",
        "            # 将参考特征与运动特征相加\n",
        "            s_r_d_t = s_r + r_d[:, t]  # s_r_d_t shape: (1, dim_w)\n",
        "            \n",
        "            # 使用运动自编码器的解码器生成图像\n",
        "            img_t, _ = self.motion_autoencoder.dec(s_r_d_t, alpha=None, feats=s_r_feats)\n",
        "            # img_t shape: (1, 3, 512, 512) - 生成的第t帧图像\n",
        "            \n",
        "            batch_frames.append(img_t)\n",
        "        \n",
        "        # 在GPU上stack这个小批次，然后立即移到CPU释放GPU内存\n",
        "        batch_tensor = torch.stack(batch_frames, dim=1)\n",
        "        # batch_tensor shape: (1, batch_size, 3, 512, 512)\n",
        "        \n",
        "        d_hat_list.append(batch_tensor.cpu())  # 移到CPU释放GPU内存\n",
        "        \n",
        "        # 清理GPU内存和临时变量\n",
        "        del batch_frames, batch_tensor\n",
        "        torch.cuda.empty_cache()\n",
        "    \n",
        "    print(\"开始在CPU上合并所有批次...\")\n",
        "    # 在CPU上合并所有批次\n",
        "    d_hat = torch.cat(d_hat_list, dim=1).squeeze()\n",
        "    # d_hat shape: (T, 3, 512, 512) - 最终的视频序列\n",
        "    \n",
        "    # 清理CPU内存\n",
        "    del d_hat_list\n",
        "    \n",
        "    print(f\"解码完成，最终tensor形状: {d_hat.shape}, 设备: {d_hat.device}\")\n",
        "    return {'d_hat': d_hat}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. 主执行流程和参数配置\n",
        "\n",
        "让我们看看 `generate.py` 的主执行部分：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 主执行流程\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # === 参数解析 ===\n",
        "    opt = InferenceOptions().parse()  # 解析命令行参数\n",
        "    opt.rank, opt.ngpus = 0, 1        # 设置GPU设备\n",
        "    \n",
        "    # === 创建推理代理 ===\n",
        "    agent = InferenceAgent(opt)\n",
        "    os.makedirs(opt.res_dir, exist_ok=True)  # 创建结果目录\n",
        "    \n",
        "    # === 输入文件路径 ===\n",
        "    ref_path = opt.ref_path    # 参考图像路径\n",
        "    aud_path = opt.aud_path    # 音频文件路径\n",
        "    \n",
        "    # === 生成输出文件名 ===\n",
        "    if opt.res_video_path is None:\n",
        "        # 自动生成文件名，包含时间戳和参数信息\n",
        "        video_name = os.path.splitext(os.path.basename(ref_path))[0]\n",
        "        audio_name = os.path.splitext(os.path.basename(aud_path))[0]\n",
        "        call_time = datetime.datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\")\n",
        "        \n",
        "        res_video_path = os.path.join(opt.res_dir, \n",
        "            \"%s-%s-%s-nfe%s-seed%s-acfg%s-ecfg%s-%s.mp4\" % (\n",
        "                call_time, video_name, audio_name, \n",
        "                opt.nfe, opt.seed, opt.a_cfg_scale, opt.e_cfg_scale, opt.emo\n",
        "            ))\n",
        "        # 示例文件名: \"2025-01-08T10-30-45-rachel-paparazzi-nfe10-seed15-acfg2.0-ecfg1.0-happy.mp4\"\n",
        "    else:\n",
        "        res_video_path = opt.res_video_path\n",
        "    \n",
        "    # === 执行推理 ===\n",
        "    agent.run_inference(\n",
        "        res_video_path,\n",
        "        ref_path,\n",
        "        aud_path,\n",
        "        a_cfg_scale=opt.a_cfg_scale,  # 音频引导强度，默认2.0\n",
        "        r_cfg_scale=opt.r_cfg_scale,  # 参考引导强度，默认1.0\n",
        "        e_cfg_scale=opt.e_cfg_scale,  # 情感引导强度，默认1.0\n",
        "        emo=opt.emo,                  # 情感标签，可选\n",
        "        nfe=opt.nfe,                  # ODE求解步数，默认10\n",
        "        no_crop=opt.no_crop,          # 是否跳过人脸裁剪\n",
        "        seed=opt.seed                 # 随机种子，默认15\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.1 重要参数说明\n",
        "\n",
        "以下是影响生成效果的关键参数：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 重要参数详解\n",
        "\n",
        "# === 模型配置参数 ===\n",
        "opt.input_size = 512          # 输入图像尺寸 (512x512)\n",
        "opt.fps = 25.0               # 视频帧率\n",
        "opt.sampling_rate = 16000    # 音频采样率\n",
        "opt.dim_w = 512             # 潜在特征维度\n",
        "opt.dim_e = 7               # 情感类别数量\n",
        "\n",
        "# === 引导缩放参数 ===\n",
        "opt.a_cfg_scale = 2.0       # 音频引导强度\n",
        "                            # 值越大，生成结果越符合音频内容\n",
        "                            # 范围通常在1.0-3.0之间\n",
        "\n",
        "opt.r_cfg_scale = 1.0       # 参考图像引导强度  \n",
        "                            # 值越大，生成结果越接近参考图像\n",
        "                            # 通常设为1.0\n",
        "\n",
        "opt.e_cfg_scale = 1.0       # 情感引导强度\n",
        "                            # 值越大，情感表达越明显\n",
        "                            # 范围通常在0.5-2.0之间\n",
        "\n",
        "# === 采样参数 ===\n",
        "opt.nfe = 10               # ODE求解器步数 (Number of Function Evaluations)\n",
        "                           # 步数越多，质量越高但速度越慢\n",
        "                           # 推荐范围: 10-50\n",
        "\n",
        "opt.seed = 15              # 随机种子，用于复现结果\n",
        "opt.fix_noise_seed = True  # 是否固定噪声种子\n",
        "\n",
        "# === 音频处理参数 ===\n",
        "opt.wav2vec_sec = 2.0      # 音频窗口长度（秒）\n",
        "                           # 决定每个clip包含多少帧: 2.0 * 25 = 50帧\n",
        "\n",
        "opt.num_prev_frames = 10   # 前序帧数量\n",
        "                           # 用于保持视频的时序连贯性\n",
        "\n",
        "# === 情感标签 ===\n",
        "emotion_labels = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n",
        "# 如果opt.emo为None，模型会自动从音频预测情感\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. 数据流转总结\n",
        "\n",
        "让我们总结整个推理过程中的数据形状变化：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 完整的数据流转过程\n",
        "\n",
        "\"\"\"\n",
        "=== 输入阶段 ===\n",
        "参考图像文件 (.jpg/.png) -> cv2.imread() -> numpy.ndarray (H, W, 3)\n",
        "音频文件 (.wav/.mp3) -> librosa.load() -> numpy.ndarray (audio_length,)\n",
        "\n",
        "=== 预处理阶段 ===\n",
        "图像预处理:\n",
        "  原始图像 (H, W, 3) \n",
        "  -> 人脸检测和裁剪 -> (512, 512, 3)\n",
        "  -> 归一化和转tensor -> (1, 3, 512, 512)\n",
        "\n",
        "音频预处理:\n",
        "  原始音频 (audio_length,)\n",
        "  -> Wav2Vec2预处理 -> (1, audio_length)\n",
        "\n",
        "=== FLOAT模型推理阶段 ===\n",
        "\n",
        "1. 图像编码:\n",
        "   参考图像 (1, 3, 512, 512) -> 运动自编码器编码器\n",
        "   -> s_r (1, 512) + r_s_lambda (1, 512) + s_r_feats (list)\n",
        "\n",
        "2. 身份方向计算:\n",
        "   r_s_lambda (1, 512) -> 方向网络 -> r_s (1, 512)\n",
        "\n",
        "3. 音频编码:\n",
        "   音频 (1, audio_length) -> AudioEncoder\n",
        "   -> wa (1, T, 512)  # T = ceil(audio_length * fps / sampling_rate)\n",
        "\n",
        "4. 情感编码:\n",
        "   音频 (1, audio_length) -> EmotionEncoder\n",
        "   -> we (1, 1, 7)  # 7个情感类别的概率分布\n",
        "\n",
        "5. 流匹配采样 (逐块处理):\n",
        "   每个clip:\n",
        "     初始噪声 (1, 50, 512) -> ODE求解器 -> 运动特征 (1, 50, 512)\n",
        "   所有clips拼接 -> 完整运动序列 (1, T, 512)\n",
        "\n",
        "6. 解码 (分批处理):\n",
        "   对每帧t:\n",
        "     s_r + r_d[:, t] -> 运动自编码器解码器 -> 图像帧 (1, 3, 512, 512)\n",
        "   所有帧拼接 -> 视频序列 (T, 3, 512, 512)\n",
        "\n",
        "=== 输出阶段 ===\n",
        "视频tensor (T, 3, 512, 512) \n",
        "-> 格式转换和缩放 -> (T, 512, 512, 3), uint8, [0,255]\n",
        "-> torchvision.io.write_video() -> 临时MP4文件\n",
        "-> ffmpeg音视频合成 -> 最终MP4文件\n",
        "\n",
        "=== 关键维度说明 ===\n",
        "- T: 视频总帧数，由音频长度和帧率决定\n",
        "- 512: 潜在特征维度 (dim_w)\n",
        "- 50: 每个clip的帧数 (wav2vec_sec * fps = 2.0 * 25)\n",
        "- 10: 前序帧数量 (num_prev_frames)\n",
        "- 7: 情感类别数量 (dim_e)\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. 使用示例\n",
        "\n",
        "以下是如何使用这个推理脚本的具体示例：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "bash"
        }
      },
      "outputs": [],
      "source": [
        "# 基本使用命令\n",
        "python generate.py \\\n",
        "    --ref_path \"./assets/rachel.webp\" \\\n",
        "    --aud_path \"./assets/paparazzi.wav\" \\\n",
        "    --res_dir \"./results\" \\\n",
        "    --nfe 10 \\\n",
        "    --seed 15 \\\n",
        "    --a_cfg_scale 2.0 \\\n",
        "    --e_cfg_scale 1.0 \\\n",
        "    --emo \"happy\"\n",
        "\n",
        "# 高质量生成（更多ODE步数）\n",
        "python generate.py \\\n",
        "    --ref_path \"./assets/sam_altman.webp\" \\\n",
        "    --aud_path \"./assets/aud-sample-vs-1.wav\" \\\n",
        "    --nfe 20 \\\n",
        "    --a_cfg_scale 2.5\n",
        "\n",
        "# 跳过人脸裁剪（如果图像已经是正确格式）\n",
        "python generate.py \\\n",
        "    --ref_path \"./assets/preprocessed_face.jpg\" \\\n",
        "    --aud_path \"./assets/speech.wav\" \\\n",
        "    --no_crop \\\n",
        "    --emo \"neutral\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. 性能优化和内存管理\n",
        "\n",
        "代码中采用了多种优化策略来处理大型视频生成：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 性能优化策略\n",
        "\n",
        "\"\"\"\n",
        "1. 分块采样 (Chunk-based Sampling):\n",
        "   - 将长音频分割为2秒的片段进行处理\n",
        "   - 每个片段生成50帧 (2秒 × 25fps)\n",
        "   - 使用前序帧保持时序连贯性\n",
        "   - 优势: 减少GPU内存占用，支持任意长度音频\n",
        "\n",
        "2. 分批解码 (Batch Decoding):\n",
        "   - 解码阶段按批次处理帧（默认50帧一批）\n",
        "   - 每批处理完立即转移到CPU\n",
        "   - 清理GPU内存缓存\n",
        "   - 优势: 防止长视频解码时GPU内存溢出\n",
        "\n",
        "3. 内存管理:\n",
        "   - 及时删除不需要的中间变量\n",
        "   - 使用torch.cuda.empty_cache()清理GPU缓存\n",
        "   - CPU和GPU之间合理的数据转移\n",
        "   - 优势: 支持生成更长的视频序列\n",
        "\n",
        "4. 预处理优化:\n",
        "   - 人脸检测只在较小分辨率下进行\n",
        "   - 图像变换使用高效的albumentations库\n",
        "   - 音频预处理使用预训练的Wav2Vec2\n",
        "   - 优势: 提高预处理速度和准确性\n",
        "\n",
        "5. 推理优化:\n",
        "   - 使用@torch.no_grad()装饰器禁用梯度计算\n",
        "   - 模型设置为eval()模式\n",
        "   - ODE求解器使用高效的数值方法\n",
        "   - 优势: 减少内存占用，提高推理速度\n",
        "\"\"\"\n",
        "\n",
        "# 内存使用估算\n",
        "def estimate_memory_usage(T, batch_size=50):\n",
        "    \"\"\"\n",
        "    估算内存使用量\n",
        "    T: 视频帧数\n",
        "    batch_size: 批处理大小\n",
        "    \"\"\"\n",
        "    # 主要内存占用组件\n",
        "    image_tensor_mb = T * 3 * 512 * 512 * 4 / (1024**2)  # 视频序列\n",
        "    audio_features_mb = T * 512 * 4 / (1024**2)          # 音频特征\n",
        "    motion_features_mb = T * 512 * 4 / (1024**2)         # 运动特征\n",
        "    \n",
        "    total_mb = image_tensor_mb + audio_features_mb + motion_features_mb\n",
        "    \n",
        "    print(f\"视频帧数: {T}\")\n",
        "    print(f\"视频序列内存: {image_tensor_mb:.1f} MB\")\n",
        "    print(f\"音频特征内存: {audio_features_mb:.1f} MB\") \n",
        "    print(f\"运动特征内存: {motion_features_mb:.1f} MB\")\n",
        "    print(f\"总估算内存: {total_mb:.1f} MB\")\n",
        "    \n",
        "    return total_mb\n",
        "\n",
        "# 示例：10秒音频的内存使用\n",
        "T_10s = int(10 * 25)  # 10秒 × 25fps = 250帧\n",
        "estimate_memory_usage(T_10s)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. 总结\n",
        "\n",
        "本notebook详细解析了FLOAT模型的推理代码 `generate.py`，包括：\n",
        "\n",
        "### 主要组件\n",
        "1. **DataProcessor**: 负责图像和音频的预处理\n",
        "2. **InferenceAgent**: 负责模型加载和推理执行\n",
        "3. **FLOAT模型**: 核心的生成模型，包含编码器、流匹配变换器和解码器\n",
        "\n",
        "### 关键技术\n",
        "- **流匹配 (Flow Matching)**: 用于生成连续的运动序列\n",
        "- **分块采样**: 处理长音频序列的高效策略\n",
        "- **多模态条件**: 结合音频、参考图像和情感信息\n",
        "- **分类器自由引导**: 通过cfg_scale参数控制生成质量\n",
        "\n",
        "### 数据流转\n",
        "```\n",
        "输入文件 -> 预处理 -> 编码 -> 流匹配采样 -> 解码 -> 输出视频\n",
        "```\n",
        "\n",
        "### 形状变化总结\n",
        "- 参考图像: `文件` → `(H,W,3)` → `(1,3,512,512)` → `(1,512)`\n",
        "- 音频: `文件` → `(audio_length,)` → `(1,audio_length)` → `(1,T,512)`\n",
        "- 生成结果: `(1,T,512)` → `(T,3,512,512)` → `MP4文件`\n",
        "\n",
        "这个推理脚本展现了现代深度学习模型在音频驱动视频生成任务中的完整工作流程，包含了数据处理、模型推理、内存管理等多个方面的最佳实践。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
